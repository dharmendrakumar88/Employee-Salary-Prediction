{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Import numpy for isnan check\n",
        "import os # Import os to check file existence\n",
        "\n",
        "\n",
        "# Load the data\n",
        "file_path = r\"/content/adult 3.csv\"\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Error: The data file '{file_path}' was not found.\")\n",
        "    print(\"Please upload the 'adult 3.csv' file to the /content/ directory and run this cell again.\")\n",
        "    exit() # Exit if data not found\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(file_path)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data from '{file_path}': {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "print(\"Original data shape:\", data.shape)\n",
        "\n",
        "# --- Data Cleaning and Preprocessing ---\n",
        "\n",
        "# Replace '?' with 'Other' and remove specific categories\n",
        "data['workclass'] = data['workclass'].replace({'?':\"Other\"})\n",
        "data = data[data['workclass']!='Without-pay']\n",
        "data = data[data['workclass']!='Never-worked']\n",
        "\n",
        "# Remove specific education levels\n",
        "# Keep 'education' for now as it's in the list of desired features, but clean it\n",
        "data = data[data['education']!='5th-6th']\n",
        "data = data[data['education']!='1st-4th']\n",
        "data = data[data['education']!='Preschool']\n",
        "\n",
        "\n",
        "# Replace '?' with 'Other' and remove specific occupation\n",
        "data['occupation'] = data['occupation'].replace({'?':\"Other\"})\n",
        "data = data[data['occupation']!='Armed-Forces']\n",
        "\n",
        "# Replace '?' with 'Other' and remove specific native-country\n",
        "data[\"native-country\"] = data[\"native-country\"].replace({'?':\"Other\"})\n",
        "data = data[data['native-country']!='Holand-Netherlands']\n",
        "\n",
        "print(\"Shape after initial cleaning:\", data.shape)\n",
        "\n",
        "\n",
        "# Function to remove outliers using IQR\n",
        "def remove_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    print(f\"{column}: Keeping values between {lower_bound:.2f} and {upper_bound:.2f}\")\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "# Remove outliers from specified numerical columns\n",
        "data = remove_outliers_iqr(data, 'age')\n",
        "data = remove_outliers_iqr(data, 'hours-per-week')\n",
        "\n",
        "print(\"Shape after outlier removal:\", data.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Feature Engineering / Selection ---\n",
        "\n",
        "# Removed the line dropping 'education' as it's a desired feature now\n",
        "# data = data.drop(columns=['education'])\n",
        "# print(\"Shape after dropping 'education':\", data.shape)\n",
        "\n",
        "\n",
        "# --- Encoding Categorical Features ---\n",
        "\n",
        "# Identify categorical columns to encode (including 'education' and excluding 'income' for now)\n",
        "# Ensure only the desired categorical features are encoded\n",
        "categorical_cols_to_encode = ['education', 'occupation', 'gender', 'race', 'native-country'] # Removed 'workclass', 'marital-status', 'relationship' as they are not in the desired list\n",
        "\n",
        "# Create and fit LabelEncoders for each categorical column\n",
        "encoders = {}\n",
        "for col in categorical_cols_to_encode:\n",
        "    if col in data.columns:\n",
        "        encoder = LabelEncoder()\n",
        "        # Handle potential NaN values in the column before fitting/transforming\n",
        "        # Although previous steps removed '?', check for other NaNs if necessary\n",
        "        # For simplicity, assuming no other NaNs introduced by cleaning\n",
        "        data[col] = encoder.fit_transform(data[col])\n",
        "        encoders[col] = encoder\n",
        "\n",
        "# Encode the target variable 'income' separately\n",
        "if 'income' in data.columns:\n",
        "    income_encoder = LabelEncoder()\n",
        "    data['income_encoded'] = income_encoder.fit_transform(data['income'])\n",
        "    encoders['income'] = income_encoder # Save the income encoder\n",
        "\n",
        "\n",
        "\n",
        "# --- Define Features (X) and Target (y) ---\n",
        "\n",
        "# Define the list of desired feature columns\n",
        "desired_features = ['age', 'education', 'occupation', 'gender', 'race', 'hours-per-week', 'capital-gain', 'capital-loss', 'native-country']\n",
        "\n",
        "# Features X (select only the desired columns)\n",
        "# Ensure the order of columns in X is consistent with desired_features\n",
        "X = data[desired_features]\n",
        "\n",
        "\n",
        "# Save the list of feature columns in the correct order\n",
        "feature_columns = X.columns.tolist()\n",
        "joblib.dump(feature_columns, \"feature_columns.pkl\")\n",
        "print(\"Saved feature column list as feature_columns.pkl\")\n",
        "\n",
        "\n",
        "# Target y (the encoded income column)\n",
        "y = data['income_encoded']\n",
        "\n",
        "# --- Scaling Numerical Features ---\n",
        "\n",
        "# Identify numerical columns among the desired features\n",
        "numerical_cols_to_scale = ['age', 'hours-per-week', 'capital-gain', 'capital-loss'] # Add 'capital-gain' and 'capital-loss'\n",
        "\n",
        "# Initialize and fit the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "# Apply scaling only to numerical columns within X\n",
        "X[numerical_cols_to_scale] = scaler.fit_transform(X[numerical_cols_to_scale])\n",
        "\n",
        "\n",
        "# Convert scaled X back to a DataFrame to keep column names (optional but helpful)\n",
        "X_scaled_df = X.copy() # X is already a DataFrame, just copy after scaling\n",
        "\n",
        "\n",
        "# --- Save Preprocessing Objects ---\n",
        "\n",
        "# Save the fitted scaler and encoders\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "joblib.dump(encoders, \"encoders.pkl\")\n",
        "print(\"Saved scaler as scaler.pkl and encoders as encoders.pkl\")\n",
        "\n",
        "\n",
        "# --- Train-Test Split ---\n",
        "\n",
        "# Use the scaled features and encoded target\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(X_scaled_df, y, test_size=0.2, random_state=42, stratify=y) # Using 42 for consistency\n",
        "\n",
        "\n",
        "# --- Model Training and Evaluation ---\n",
        "\n",
        "# Define models with consistent parameters\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
        "    \"RandomForest\": RandomForestClassifier(random_state=42), # Added random_state for reproducibility\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"SVM\": SVC(probability=True, random_state=42), # Added probability=True and random_state\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42) # Added random_state\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate each model\n",
        "print(\"\\n--- Model Training and Evaluation ---\")\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(xtrain, ytrain)\n",
        "    preds = model.predict(xtest)\n",
        "    acc = accuracy_score(ytest, preds)\n",
        "    results[name] = acc\n",
        "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Get the best model based on accuracy\n",
        "best_model_name = max(results, key=results.get)\n",
        "best_model = models[best_model_name]\n",
        "print(f\"\\nâœ… Best model: {best_model_name} with accuracy {results[best_model_name]:.4f}\")\n",
        "\n",
        "# --- Save the Best Model ---\n",
        "\n",
        "joblib.dump(best_model, \"best_model.pkl\")\n",
        "print(\"âœ… Saved best model as best_model.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q9YkpZhLjKk",
        "outputId": "e9a0a846-5f05-432e-e812-07df337aa192"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data shape: (48842, 15)\n",
            "Shape after initial cleaning: (47956, 15)\n",
            "age: Keeping values between -2.00 and 78.00\n",
            "hours-per-week: Keeping values between 32.50 and 52.50\n",
            "Shape after outlier removal: (34620, 15)\n",
            "Saved feature column list as feature_columns.pkl\n",
            "Saved scaler as scaler.pkl and encoders as encoders.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-4076923920.py:134: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[numerical_cols_to_scale] = scaler.fit_transform(X[numerical_cols_to_scale])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Training and Evaluation ---\n",
            "Training LogisticRegression...\n",
            "LogisticRegression Accuracy: 0.7826\n",
            "Training RandomForest...\n",
            "RandomForest Accuracy: 0.8063\n",
            "Training KNN...\n",
            "KNN Accuracy: 0.7796\n",
            "Training SVM...\n",
            "SVM Accuracy: 0.7426\n",
            "Training GradientBoosting...\n",
            "GradientBoosting Accuracy: 0.8345\n",
            "\n",
            "âœ… Best model: GradientBoosting with accuracy 0.8345\n",
            "âœ… Saved best model as best_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "import numpy as np # Import numpy for isnan check\n",
        "\n",
        "# Load the trained model, scaler, encoders, and feature column list\n",
        "try:\n",
        "    model = joblib.load(\"/content/best_model.pkl\")\n",
        "    scaler = joblib.load(\"/content/scaler.pkl\")\n",
        "    encoders = joblib.load(\"/content/encoders.pkl\")\n",
        "    feature_columns = joblib.load(\"/content/feature_columns.pkl\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    st.error(f\"Required file not found: {e}. Please ensure 'best_model.pkl', 'scaler.pkl', 'encoders.pkl', and 'feature_columns.pkl' are in the /content directory.\")\n",
        "    st.stop()\n",
        "except Exception as e:\n",
        "    st.error(f\"Error loading required files: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "st.set_page_config(page_title=\"Employee Salary Classification\", page_icon=\"ðŸ’¼\", layout=\"centered\")\n",
        "\n",
        "st.title(\"ðŸ’¼ Employee Salary Classification App\")\n",
        "st.markdown(\"Predict whether an employee earns >50K or â‰¤50K based on input features.\")\n",
        "\n",
        "# Sidebar inputs - dynamically create inputs based on the expected feature columns\n",
        "st.sidebar.header(\"Input Employee Details\")\n",
        "\n",
        "input_data = {}\n",
        "for col in feature_columns:\n",
        "    # Determine input type based on column name or expected data type\n",
        "    # These should match the selected features and their types\n",
        "    if col == 'age':\n",
        "        input_data[col] = st.sidebar.slider(\"Age\", 17, 90, 35) # Example range\n",
        "    elif col == 'hours-per-week':\n",
        "         input_data[col] = st.sidebar.slider(\"Hours per week\", 1, 99, 40) # Example range\n",
        "    elif col == 'capital-gain':\n",
        "         input_data[col] = st.sidebar.number_input(\"Capital Gain\", 0, 100000, 0) # Example range\n",
        "    elif col == 'capital-loss':\n",
        "         input_data[col] = st.sidebar.number_input(\"Capital Loss\", 0, 4500, 0) # Example range\n",
        "    elif col in encoders and col != 'income': # Categorical columns with loaded encoders (education, occupation, gender, race, native-country)\n",
        "        if col in encoders:\n",
        "            original_labels = encoders[col].classes_.tolist()\n",
        "            selected_value = st.sidebar.selectbox(col.replace('-', ' ').title(), original_labels)\n",
        "            input_data[col] = selected_value # Store original value\n",
        "        else:\n",
        "             st.warning(f\"Encoder not found for column: {col}\")\n",
        "             input_data[col] = None # Handle columns without encoders\n",
        "\n",
        "    else:\n",
        "        # Default input for any unexpected columns - should not happen with dynamic sidebar\n",
        "        st.warning(f\"Unhandled feature in sidebar generation: {col}\")\n",
        "        input_data[col] = \"\" # Or some other default/error handling\n",
        "\n",
        "\n",
        "# Convert input_data to DataFrame\n",
        "input_df = pd.DataFrame([input_data])\n",
        "\n",
        "# --- Preprocess the single input data ---\n",
        "\n",
        "# Create a copy to avoid modifying the original input_df directly\n",
        "processed_input_df = input_df.copy()\n",
        "\n",
        "# Apply label encoding to categorical columns using loaded encoders\n",
        "for col in feature_columns: # Iterate through expected feature columns\n",
        "     if col in encoders and col != 'income': # Check if it's a categorical column that needs encoding\n",
        "        if col in processed_input_df.columns: # Ensure column exists in input data\n",
        "            # Handle unseen labels during single prediction\n",
        "            if processed_input_df[col][0] in encoders[col].classes_:\n",
        "                 processed_input_df[col] = encoders[col].transform(processed_input_df[col])\n",
        "            else:\n",
        "                 st.warning(f\"Selected value '{processed_input_df[col][0]}' for '{col}' not seen in training data. Cannot encode. Setting to NaN.\")\n",
        "                 processed_input_df[col] = np.nan # Use numpy.nan instead of None\n",
        "        else:\n",
        "            st.warning(f\"Categorical feature '{col}' expected but not found in input data.\")\n",
        "            processed_input_df[col] = np.nan # Add missing categorical column as NaN\n",
        "\n",
        "\n",
        "# Ensure processed_input_df has the same columns and order as feature_columns\n",
        "# This step is crucial if the input DataFrame might be missing columns\n",
        "for col in feature_columns:\n",
        "    if col not in processed_input_df.columns:\n",
        "        # Add missing numerical columns with a default (e.g., 0 or mean from training)\n",
        "        # For simplicity, using 0 here, but consider using the mean/median from training data\n",
        "        if col in ['age', 'hours-per-week', 'capital-gain', 'capital-loss']:\n",
        "             processed_input_df[col] = 0\n",
        "        # Missing categorical columns were handled above by adding np.nan\n",
        "\n",
        "# Ensure column order matches\n",
        "processed_input_df = processed_input_df[feature_columns]\n",
        "\n",
        "# Fill any remaining NaNs that might have been introduced during processing (e.g., from unseen categories)\n",
        "processed_input_df.fillna(0, inplace=True) # Simple fill with 0, adjust as needed\n",
        "\n",
        "\n",
        "# Apply scaling to the processed input DataFrame using the loaded scaler\n",
        "try:\n",
        "    # Ensure the input DataFrame has the correct numerical columns for scaling\n",
        "    numerical_cols_to_scale = ['age', 'hours-per-week', 'capital-gain', 'capital-loss']\n",
        "    input_scaled_numerical = scaler.transform(processed_input_df[numerical_cols_to_scale])\n",
        "    # Replace original numerical columns with scaled ones\n",
        "    processed_input_df[numerical_cols_to_scale] = input_scaled_numerical\n",
        "\n",
        "    input_scaled_df = processed_input_df.copy() # Renaming for clarity\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    st.error(f\"Error during scaling single input data: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "\n",
        "st.write(\"### ðŸ”Ž Processed Input Data (after encoding and scaling)\")\n",
        "st.write(input_scaled_df)\n",
        "\n",
        "# Predict button\n",
        "if st.button(\"Predict Salary Class\"):\n",
        "    # Make prediction using the scaled input data\n",
        "    try:\n",
        "        prediction_encoded = model.predict(input_scaled_df)\n",
        "\n",
        "        # Decode the prediction using the income encoder\n",
        "        if 'income' in encoders:\n",
        "            prediction = encoders['income'].inverse_transform(prediction_encoded)\n",
        "            st.success(f\"âœ… Predicted Salary Class: {prediction[0]}\")\n",
        "        else:\n",
        "            st.warning(\"Income encoder not found. Displaying raw prediction.\")\n",
        "            st.success(f\"âœ… Raw Prediction: {prediction_encoded[0]}\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error during prediction: {e}\")\n",
        "\n",
        "\n",
        "# Batch prediction\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"#### ðŸ“‚ Batch Prediction\")\n",
        "uploaded_file = st.file_uploader(\"Upload a CSV file for batch prediction\", type=\"csv\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    try:\n",
        "        batch_data = pd.read_csv(uploaded_file)\n",
        "        st.write(\"Uploaded data preview:\", batch_data.head())\n",
        "\n",
        "        # --- Preprocess the batch data ---\n",
        "        # Apply the same preprocessing steps as the training data\n",
        "\n",
        "        # Create a copy to avoid modifying the original batch_data DataFrame\n",
        "        processed_batch_data = batch_data.copy()\n",
        "\n",
        "        # Handle missing values ('?') - ensure these columns exist in the batch data before processing\n",
        "        if 'workclass' in processed_batch_data.columns:\n",
        "             processed_batch_data['workclass'].replace({'?':\"Other\"},inplace=True)\n",
        "        if 'occupation' in processed_batch_data.columns:\n",
        "             processed_batch_data['occupation'].replace({'?':\"Other\"},inplace=True)\n",
        "        if 'native-country' in processed_batch_data.columns:\n",
        "             processed_batch_data[\"native-country\"].replace({'?':\"Other\"},inplace=True)\n",
        "\n",
        "        # Drop rows based on conditions (mirroring training) - ensure these columns exist in the batch data\n",
        "        initial_batch_rows = len(processed_batch_data)\n",
        "        # Check if columns exist before filtering\n",
        "        if 'workclass' in processed_batch_data.columns:\n",
        "             processed_batch_data = processed_batch_data[processed_batch_data['workclass']!='Without-pay']\n",
        "             processed_batch_data = processed_batch_data[processed_batch_data['workclass']!='Never-worked']\n",
        "        if 'education' in processed_batch_data.columns: # Keep 'education' in batch processing as it's now a feature\n",
        "             processed_batch_data = processed_batch_data[processed_batch_data['education']!='5th-6th']\n",
        "             processed_batch_data = processed_batch_data[processed_batch_data['education']!='1st-4th']\n",
        "             processed_batch_data = processed_batch_data[processed_batch_data['education']!='Preschool']\n",
        "        if 'occupation' in processed_batch_data.columns:\n",
        "             processed_batch_data = processed_batch_data[processed_batch_data['occupation']!='Armed-Forces']\n",
        "        if 'native-country' in processed_batch_data.columns:\n",
        "            processed_batch_data = processed_batch_data[processed_batch_data['native-country']!='Holand-Netherlands']\n",
        "\n",
        "        if len(processed_batch_data) < initial_batch_rows:\n",
        "            st.warning(f\"Filtered out {initial_batch_rows - len(processed_batch_data)} rows during preprocessing.\")\n",
        "\n",
        "\n",
        "        # Drop columns that are NOT in the desired feature list, but ARE in the original batch data\n",
        "        columns_to_drop = [col for col in processed_batch_data.columns if col not in feature_columns and col != 'income']\n",
        "        if columns_to_drop:\n",
        "             processed_batch_data.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "\n",
        "        # Drop the original 'income' column if it exists in the batch data\n",
        "        if 'income' in processed_batch_data.columns:\n",
        "            processed_batch_data.drop(columns=['income'], inplace=True)\n",
        "\n",
        "\n",
        "        # Ensure processed_batch_data has the same columns as feature_columns.\n",
        "        # Add missing columns with a default value (e.g., 0) and reindex.\n",
        "        for col in feature_columns:\n",
        "            if col not in processed_batch_data.columns:\n",
        "                st.warning(f\"Feature '{col}' expected but not found in batch data. Adding with default value 0.\")\n",
        "                # Decide on default value based on type - using 0 for simplicity\n",
        "                processed_batch_data[col] = 0\n",
        "\n",
        "\n",
        "        # Apply label encoding to categorical columns in batch data using loaded encoders\n",
        "        for col in feature_columns: # Iterate through expected feature columns\n",
        "            if col in encoders and col != 'income': # Check if it's a categorical column that needs encoding\n",
        "                if col in processed_batch_data.columns: # Ensure column exists in batch data\n",
        "                    # Handle unseen labels: map to a default or use a more robust strategy\n",
        "                    # Check if the value is in the encoder's classes before transforming\n",
        "                    processed_batch_data[col] = processed_batch_data[col].apply(\n",
        "                        lambda x: encoders[col].transform([x])[0] if x in encoders[col].classes_ else np.nan # Use np.nan for unseen\n",
        "                    )\n",
        "                else:\n",
        "                     st.warning(f\"Categorical feature '{col}' expected but not found in batch data.\")\n",
        "                     processed_batch_data[col] = np.nan # Add missing categorical column as NaN\n",
        "\n",
        "\n",
        "        # Ensure column order matches the training data\n",
        "        processed_batch_data = processed_batch_data[feature_columns]\n",
        "\n",
        "        # Fill any remaining NaNs that might have been introduced during processing (e.g., from unseen categories)\n",
        "        # Using 0 here, but a more sophisticated approach might use mean/median from training data\n",
        "        processed_batch_data.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "        # Apply scaling to the processed batch data using the loaded scaler\n",
        "        try:\n",
        "            # Identify numerical columns among the selected features for scaling\n",
        "            numerical_cols_to_scale = ['age', 'hours-per-week', 'capital-gain', 'capital-loss'] # Ensure this matches the training features\n",
        "\n",
        "            # Ensure these columns exist in the batch data before attempting to scale\n",
        "            cols_to_scale_present = [col for col in numerical_cols_to_scale if col in processed_batch_data.columns]\n",
        "            if cols_to_scale_present:\n",
        "                batch_scaled_numerical = scaler.transform(processed_batch_data[cols_to_scale_present])\n",
        "                # Replace original numerical columns with scaled ones\n",
        "                processed_batch_data[cols_to_scale_present] = batch_scaled_numerical\n",
        "            else:\n",
        "                 st.warning(\"No numerical columns found for scaling in batch data.\")\n",
        "\n",
        "\n",
        "            batch_scaled_df = processed_batch_data.copy() # Renaming for clarity\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error during scaling batch input data: {e}\")\n",
        "            st.stop()\n",
        "\n",
        "\n",
        "        # Make batch predictions\n",
        "        batch_preds_encoded = model.predict(batch_scaled_df)\n",
        "\n",
        "        # Decode batch predictions using the income encoder\n",
        "        if 'income' in encoders:\n",
        "            batch_preds = encoders['income'].inverse_transform(batch_preds_encoded)\n",
        "            # Add predictions to the processed_batch_data DataFrame\n",
        "            processed_batch_data['PredictedClass'] = batch_preds\n",
        "        else:\n",
        "            st.warning(\"Income encoder for 'income' not found. Displaying raw predictions.\")\n",
        "            processed_batch_data['PredictedClass'] = batch_preds_encoded\n",
        "\n",
        "\n",
        "        st.write(\"âœ… Predictions:\")\n",
        "        # Display the processed batch data with predictions\n",
        "        st.write(processed_batch_data.head())\n",
        "\n",
        "        # Provide download link for the predicted CSV\n",
        "        # Use processed_batch_data which now includes the predictions\n",
        "        csv = processed_batch_data.to_csv(index=False).encode('utf-8')\n",
        "        st.download_button(\"Download Predictions CSV\", csv, file_name='predicted_classes.csv', mime='text/csv')\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error during batch prediction process: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CUN9HwwLtWv",
        "outputId": "140a1acb-b2c1-43f2-b4b3-7873aecd3282"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok\n",
        "\n",
        "# Replace with your actual ngrok authtoken\n",
        "# You can get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "!ngrok config add-authtoken 30KRBhUe4KVIHFymzNBz8qMc2sF_4nbPCGReWtRe8Mvy2TQZB\n",
        "\n",
        "import os\n",
        "import threading\n",
        "\n",
        "def run_streamlit():\n",
        "    os.system('streamlit run app.py --server.port 8501')\n",
        "\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "#wait for a while\n",
        "time.sleep(5)\n",
        "\n",
        "#create a tunnel\n",
        "\n",
        "public_url  = ngrok.connect(8501)\n",
        "print(\"your streamlit app is live here :\",public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buXPG4CRMLnv",
        "outputId": "90b81dc6-a365-4a0b-a748-c294ae6ad5fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "your streamlit app is live here : NgrokTunnel: \"https://39c17111ab48.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PoivV_-RMxN3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}